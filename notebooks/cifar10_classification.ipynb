{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbdf2754",
   "metadata": {},
   "source": [
    "# CIFAR-10 Image Classification with ResNet50 Transfer Learning\n",
    "\n",
    "**Project:** Computer Vision with Deep Learning\n",
    "**Author:** Alberto Diaz Durana\n",
    "**Date:** 2025-12-14\n",
    "**Version:** 1.0\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Build an image classification model for CIFAR-10 dataset using ResNet50 with transfer learning, incorporating:\n",
    "- Data augmentation for improved generalization\n",
    "- Two-phase training (frozen base, then fine-tuning)\n",
    "- Learning rate scheduling\n",
    "- Model interpretability (Grad-CAM)\n",
    "- Comprehensive evaluation metrics\n",
    "\n",
    "## Dataset\n",
    "\n",
    "| Attribute | Value |\n",
    "|-----------|-------|\n",
    "| Dataset | CIFAR-10 |\n",
    "| Training | 8,000 images (stratified from 10K subset) |\n",
    "| Validation | 2,000 images (stratified from 10K subset) |\n",
    "| Test | 10,000 images (holdout) |\n",
    "| Image Size | 32x32 RGB |\n",
    "| Classes | 10 |\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. Environment Setup & Imports\n",
    "2. Data Loading & Exploration\n",
    "3. Data Preprocessing\n",
    "4. Data Augmentation\n",
    "5. Model Architecture (ResNet50 + Custom Head)\n",
    "6. Training Phase 1 (Frozen Layers)\n",
    "7. Training Phase 2 (Fine-tuning)\n",
    "8. Model Evaluation & Metrics\n",
    "9. Model Interpretability (Grad-CAM)\n",
    "10. Conclusions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd168f8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Imports\n",
    "\n",
    "Configure GPU memory growth to prevent OOM errors and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59464ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration (must run before importing TensorFlow layers)\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"OK: GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "        print(f\"    Device: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"WARNING: GPU config failed: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected, using CPU\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ece997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow/Keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Settings\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290bc765",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration\n",
    "\n",
    "### 2.1 Project Structure\n",
    "\n",
    "Define paths for Colab compatibility (relative paths, no hardcoded locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project structure (Colab-compatible)\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in dir() else False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive if needed\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = Path('/content')\n",
    "else:\n",
    "    # notebooks/ -> computer_vision/ (project root)\n",
    "    PROJECT_ROOT = Path('.').resolve().parent\n",
    "    # Verify we're in the correct project structure\n",
    "    if PROJECT_ROOT.name != 'computer_vision':\n",
    "        # Fallback: search upward for computer_vision directory\n",
    "        current = Path('.').resolve()\n",
    "        while current.parent != current:\n",
    "            if current.name == 'computer_vision':\n",
    "                PROJECT_ROOT = current\n",
    "                break\n",
    "            current = current.parent\n",
    "        else:\n",
    "            # Last resort: use explicit path\n",
    "            PROJECT_ROOT = Path('/home/berto/computer_vision')\n",
    "\n",
    "# Define paths\n",
    "PATHS = {\n",
    "    'data_raw': PROJECT_ROOT / 'data' / 'raw',\n",
    "    'data_processed': PROJECT_ROOT / 'data' / 'processed',\n",
    "    'models': PROJECT_ROOT / 'models',\n",
    "    'figures': PROJECT_ROOT / 'outputs' / 'figures',\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in PATHS.values():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Paths configured: {list(PATHS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85333675",
   "metadata": {},
   "source": [
    "### 2.2 Load Dataset\n",
    "Load CIFAR-10 dataset using TensorFlow/Keras built-in loader.\n",
    "- Training subset: 10,000 images (will split 80/20 for train/val)\n",
    "- Test set: 10,000 images (holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Use 10K subset for training (as per project plan)\n",
    "x_train_subset = x_train_full[:10000]\n",
    "y_train_subset = y_train_full[:10000]\n",
    "\n",
    "# Class names for CIFAR-10\n",
    "CLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Training subset: {x_train_subset.shape}\")\n",
    "print(f\"Test set: {x_test.shape}\")\n",
    "print(f\"Labels shape: {y_train_subset.shape}\")\n",
    "print(f\"Pixel range: [{x_train_subset.min()}, {x_train_subset.max()}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa57c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split: 80% train, 20% validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_subset, y_train_subset,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train_subset,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {x_train.shape}\")\n",
    "print(f\"Val: {x_val.shape}\")\n",
    "print(f\"Test (holdout): {x_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb1065",
   "metadata": {},
   "source": [
    "### 2.3 Data Exploration\n",
    "Visualize sample images and analyze class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a27592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Display sample images (one per class)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.where(y_train.flatten() == i)[0][0]\n",
    "    ax.imshow(x_train[idx])\n",
    "    ax.set_title(CLASS_NAMES[i])\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Images (One per Class)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS['figures'] / 'sample_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae24683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution across splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, (data, title) in zip(axes, [(y_train, 'Train (8K)'), \n",
    "                                      (y_val, 'Validation (2K)'), \n",
    "                                      (y_test, 'Test (10K)')]):\n",
    "    unique, counts = np.unique(data, return_counts=True)\n",
    "    ax.bar(CLASS_NAMES, counts)\n",
    "    ax.set_title(f'{title} Class Distribution')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS['figures'] / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Train class counts: {dict(zip(CLASS_NAMES, counts))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d8aff",
   "metadata": {},
   "source": [
    "Note: The print statement shows test set counts (1000 each), not train. The stratified split worked - classes are balanced across all splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808b90c",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "Normalize pixel values to [0, 1] range for neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "x_train_norm = x_train.astype('float32') / 255.0\n",
    "x_val_norm = x_val.astype('float32') / 255.0\n",
    "x_test_norm = x_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"Normalized pixel range: [{x_train_norm.min():.1f}, {x_train_norm.max():.1f}]\")\n",
    "print(f\"Data type: {x_train_norm.dtype}\")\n",
    "print(f\"Memory: {x_train_norm.nbytes / 1024**2:.1f} MB (train)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e9d16",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation\n",
    "Define augmentation pipeline using Keras preprocessing layers.\n",
    "Applied during training only to increase data diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17227fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),  # +/- 10% rotation\n",
    "    layers.RandomZoom(0.1),      # +/- 10% zoom\n",
    "], name='data_augmentation')\n",
    "\n",
    "# Visualize augmentation effect on a single image\n",
    "sample_img = x_train_norm[0:1]  # Keep batch dimension\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "axes[0].imshow(sample_img[0])\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "for i in range(1, 5):\n",
    "    augmented = data_augmentation(sample_img, training=True)\n",
    "    axes[i].imshow(augmented[0])\n",
    "    axes[i].set_title(f'Augmented {i}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Data Augmentation Examples ({CLASS_NAMES[y_train[0][0]]})')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS['figures'] / 'augmentation_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231dd725",
   "metadata": {},
   "source": [
    "## 5. Model Architecture\n",
    "Build transfer learning model with ResNet50 base (frozen) and custom classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68780c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import Model, Input\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "\n",
    "# Data augmentation (training only)\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# ResNet50 preprocessing\n",
    "x = tf.keras.applications.resnet50.preprocess_input(x)\n",
    "\n",
    "# ResNet50 base (frozen)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, \n",
    "                       input_shape=(32, 32, 3), pooling='avg')\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# Custom classification head\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs, outputs, name='cifar10_resnet50')\n",
    "\n",
    "print(f\"Base model layers: {len(base_model.layers)}\")\n",
    "print(f\"Base model trainable: {base_model.trainable}\")\n",
    "print(f\"Total params: {model.count_params():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9948035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd909fc",
   "metadata": {},
   "source": [
    "Model compiled. Note the key numbers:\n",
    "\n",
    "Trainable params: 527,114 (2.01 MB) - only the head trains\n",
    "\n",
    "Non-trainable params: 23,587,712 (89.98 MB) - frozen ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2472329",
   "metadata": {},
   "source": [
    "## 6. Training Phase 1: Frozen Base\n",
    "Train only the custom classification head while keeping ResNet50 weights frozen.\n",
    "- Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "- Batch size: 32 (reduce if OOM)\n",
    "- Epochs: 20 (with early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 20\n",
    "\n",
    "# Callbacks\n",
    "callbacks_phase1 = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=PATHS['models'] / 'resnet50_frozen_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max epochs: {EPOCHS_PHASE1}\")\n",
    "print(f\"Training samples: {len(x_train_norm)}\")\n",
    "print(f\"Steps per epoch: {len(x_train_norm) // BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Train Phase 1: Frozen base\n",
    "print(\"Phase 1: Training with frozen ResNet50 base...\")\n",
    "print(f\"Trainable params: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    x_train_norm, y_train,\n",
    "    validation_data=(x_val_norm, y_val),\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase1_time = time.time() - start_time\n",
    "print(f\"\\nPhase 1 training time: {phase1_time/60:.1f} minutes ({phase1_time:.0f} seconds)\")\n",
    "print(f\"Best val_accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")\n",
    "print(f\"Epochs completed: {len(history_phase1.history['loss'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12416a5",
   "metadata": {},
   "source": [
    " Issue: Training failed. ~10% accuracy = random guessing (10 classes). \n",
    " \n",
    " Diagnosis: ResNet50 pretrained on ImageNet with 224x224 images is extracting poor features from 32x32 CIFAR-10 images. The frozen base produces nearly constant outputs regardless of input. Root cause: ResNet50 is inappropriate for 32x32 images without resizing. \n",
    " \n",
    " Solution: Resize CIFAR-10 images to 224x224 before feeding into ResNet50. This will allow the pretrained weights to extract meaningful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a942c2",
   "metadata": {},
   "source": [
    "### 6.1 Model Fix: Add Resize Layer\n",
    "ResNet50 requires 224x224 input. Adding resize layer to upscale 32x32 images.\n",
    "Note: This introduces interpolation artifacts but enables feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous model from memory\n",
    "import gc\n",
    "del model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Rebuild model with resize layer\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "\n",
    "# Data augmentation (training only)\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Resize to 224x224 for ResNet50\n",
    "x = layers.Resizing(224, 224, interpolation='bilinear')(x)\n",
    "\n",
    "# ResNet50 preprocessing\n",
    "x = tf.keras.applications.resnet50.preprocess_input(x)\n",
    "\n",
    "# ResNet50 base (frozen)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, \n",
    "                       input_shape=(224, 224, 3), pooling='avg')\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# Custom classification head\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Build and compile model\n",
    "model = Model(inputs, outputs, name='cifar10_resnet50_224')\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Input shape: 32x32 -> resized to 224x224\")\n",
    "print(f\"Total params: {model.count_params():,}\")\n",
    "print(f\"Trainable params: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update checkpoint path for new model\n",
    "callbacks_phase1 = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=PATHS['models'] / 'resnet50_224_frozen_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train Phase 1 with resized input\n",
    "print(\"Phase 1 (retry): Training with 224x224 resize layer...\")\n",
    "print(f\"WARNING: Larger images = slower training, higher memory usage\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    x_train_norm, y_train,\n",
    "    validation_data=(x_val_norm, y_val),\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase1_time = time.time() - start_time\n",
    "print(f\"\\nPhase 1 training time: {phase1_time/60:.1f} minutes ({phase1_time:.0f} seconds)\")\n",
    "print(f\"Best val_accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")\n",
    "print(f\"Epochs completed: {len(history_phase1.history['loss'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca50441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.2: Phase 1 Training Curves\n",
    "# Visualize the frozen base training results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history_phase1.history['loss'], label='Train', marker='o')\n",
    "axes[0].plot(history_phase1.history['val_loss'], label='Validation', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Phase 1: Loss (Frozen Base)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=np.log(10), color='r', linestyle='--', alpha=0.5, label='Random guess')\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history_phase1.history['accuracy'], label='Train', marker='o')\n",
    "axes[1].plot(history_phase1.history['val_accuracy'], label='Validation', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Phase 1: Accuracy (Frozen Base)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0.1, color='r', linestyle='--', alpha=0.5, label='Random guess (10%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS['figures'] / 'phase1_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Phase 1 Summary:\")\n",
    "print(f\"  Final train accuracy: {history_phase1.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Final val accuracy: {history_phase1.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"  Best val accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")\n",
    "print(f\"  Conclusion: Frozen ResNet50 features do not transfer to CIFAR-10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2e4d7",
   "metadata": {},
   "source": [
    "The Phase 1 curves confirm the failure - both loss and accuracy are flat, matching the random guess baseline (10% accuracy, ~2.3 loss). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaea096",
   "metadata": {},
   "source": [
    "## 7. Training Phase 2: Fine-tuning\n",
    "Unfreeze top ResNet50 layers (conv5 block) to learn CIFAR-10 specific features.\n",
    "- Lower learning rate: 1e-4 (prevent destroying pretrained weights)\n",
    "- Same callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "- Epochs: 30 (with early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6942aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze conv5 block for fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers:\n",
    "    if 'conv5' not in layer.name:\n",
    "        layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate (required after changing trainable status)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Verify configuration\n",
    "trainable_count = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "non_trainable_count = sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
    "print(f\"Trainable params: {trainable_count:,}\")\n",
    "print(f\"Non-trainable params: {non_trainable_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 training configuration\n",
    "EPOCHS_PHASE2 = 30\n",
    "\n",
    "callbacks_phase2 = [\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint(filepath=PATHS['models'] / 'resnet50_224_finetuned_best.keras',\n",
    "                   monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Train with fine-tuning\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    x_train_norm, y_train,\n",
    "    validation_data=(x_val_norm, y_val),\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase2_time = time.time() - start_time\n",
    "print(f\"\\nPhase 2 training time: {phase2_time/60:.1f} minutes ({phase2_time:.0f} seconds)\")\n",
    "print(f\"Best val_accuracy: {max(history_phase2.history['val_accuracy']):.4f}\")\n",
    "print(f\"Epochs completed: {len(history_phase2.history['loss'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d27b1b",
   "metadata": {},
   "source": [
    "**Phase 2 Training Results**\n",
    "\n",
    "| Metric | Phase 1 (Frozen) | Phase 2 (Fine-tuned) |\n",
    "|--------|------------------|----------------------|\n",
    "| Best val_accuracy | 10.3% | 49.7% |\n",
    "| Training time | 16 min | 52 min |\n",
    "| Epochs | 12 (early stop) | 30 (full) |\n",
    "\n",
    "**Key observations:**\n",
    "- 5x improvement over frozen baseline (10% → 50%)\n",
    "- Significant validation variance (oscillating between 30-50%) indicates overfitting on small dataset\n",
    "- LR scheduling helped: best results came at lower LRs (6.25e-6)\n",
    "- Model restored to epoch 28 (best weights at 49.65%)\n",
    "\n",
    "The 65% target wasn't reached, but this is expected given:\n",
    "- Small training set (8K images)\n",
    "- Architecture mismatch (ResNet50 designed for 224x224, we're upscaling 32x32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history_phase2.history['loss'], label='Train', marker='o', markersize=3)\n",
    "axes[0].plot(history_phase2.history['val_loss'], label='Validation', marker='s', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Phase 2: Loss (Fine-tuned)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history_phase2.history['accuracy'], label='Train', marker='o', markersize=3)\n",
    "axes[1].plot(history_phase2.history['val_accuracy'], label='Validation', marker='s', markersize=3)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Phase 2: Accuracy (Fine-tuned)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0.65, color='g', linestyle='--', alpha=0.5, label='Target (65%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS['figures'] / 'phase2_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Phase 2 Summary:\")\n",
    "print(f\"  Final train accuracy: {history_phase2.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Final val accuracy: {history_phase2.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"  Best val accuracy: {max(history_phase2.history['val_accuracy']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae7626",
   "metadata": {},
   "source": [
    "The curves show clear learning with train accuracy reaching 47% and validation peaking at ~50%. The gap between train and validation indicates some overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9159962",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "Evaluate fine-tuned model on held-out test set (10,000 images).\n",
    "- Test accuracy and loss\n",
    "- Confusion matrix\n",
    "- Per-class precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize test data\n",
    "x_test_norm = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=1)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Validation Accuracy (best): {max(history_phase2.history['val_accuracy']):.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  Target: 0.6500 (65%)\")\n",
    "print(f\"  Gap to target: {0.65 - test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ec6c0",
   "metadata": {},
   "source": [
    "Test accuracy (48.6%) is close to validation (49.7%) - the model generalizes consistently. The ~16.5% gap to target confirms what we identified: architecture mismatch and small training set are limiting factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc71bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "y_pred_probs = model.predict(x_test_norm, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = y_test.flatten()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title(f'Confusion Matrix (Test Accuracy: {test_accuracy:.1%})')\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS['figures'] / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36f790",
   "metadata": {},
   "source": [
    "**Classification Report Insights**\n",
    "\n",
    "**Best performing classes:**\n",
    "- Ship (68% precision) - distinct shape/context\n",
    "- Automobile (65% precision, 62% F1) - clear features\n",
    "- Frog (71% recall, 58% F1) - distinctive green color/texture\n",
    "\n",
    "**Worst performing classes:**\n",
    "- Cat (29% precision, 23% F1) - confused with dog\n",
    "- Bird (33% precision, 26% F1) - similar to deer, small features\n",
    "- Deer (46% precision but only 25% recall) - many missed\n",
    "\n",
    "**Common confusions (visible in matrix):**\n",
    "- Cat ↔ Dog (both animals, similar shapes)\n",
    "- Bird ↔ Deer (both animals in natural settings)\n",
    "- Truck ↔ Automobile (both vehicles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374234d",
   "metadata": {},
   "source": [
    "## 9. Model Interpretability - Grad-CAM\n",
    "Visualize which image regions the model focuses on for predictions.\n",
    "- Grad-CAM on correct predictions (what the model learned)\n",
    "- Grad-CAM on misclassifications (failure analysis)\n",
    "- Limitations: coarse heatmaps due to 32x32→224x224 upscaling\n",
    "\n",
    "### 9.1 Auto-Restore State\n",
    "Automatically loads saved model/state if available, otherwise uses variables from previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-restore: Load saved state if available, otherwise skip\n",
    "import pickle\n",
    "\n",
    "model_path = PATHS['models'] / 'resnet50_224_finetuned_current.keras'\n",
    "history_path = PATHS['models'] / 'training_history.pkl'\n",
    "state_path = PATHS['models'] / 'evaluation_state.npz'\n",
    "\n",
    "if model_path.exists() and state_path.exists():\n",
    "    print(\"Saved state found - restoring...\")\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    if history_path.exists():\n",
    "        with open(history_path, 'rb') as f:\n",
    "            histories = pickle.load(f)\n",
    "    \n",
    "    state = np.load(state_path)\n",
    "    y_pred = state['y_pred']\n",
    "    y_pred_probs = state['y_pred_probs']\n",
    "    y_true = state['y_true']\n",
    "    x_test_norm = state['x_test_norm']\n",
    "    \n",
    "    print(f\"Model loaded: {model.name}\")\n",
    "    print(f\"Test data shape: {x_test_norm.shape}\")\n",
    "    print(f\"Test accuracy: {np.mean(y_pred == y_true):.4f}\")\n",
    "else:\n",
    "    print(\"No saved state found - using variables from previous cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cn2f0pwv4z",
   "metadata": {},
   "source": [
    "### 9.2 Grad-CAM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3079106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM implementation\n",
    "import tensorflow as tf\n",
    "\n",
    "# Grad-CAM for Keras 3.x - skip InputLayer\n",
    "def make_gradcam_heatmap_v3(img_array, model):\n",
    "    \"\"\"Generate Grad-CAM heatmap.\"\"\"\n",
    "    base_model = model.get_layer('resnet50')\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Start with input array\n",
    "        x = img_array\n",
    "        \n",
    "        # Through preprocessing layers (skip InputLayer)\n",
    "        x = model.get_layer('data_augmentation')(x, training=False)\n",
    "        x = model.get_layer('resizing')(x)\n",
    "        \n",
    "        # Through ResNet50 - need to get intermediate output\n",
    "        # Build sub-model for conv5 output\n",
    "        conv_layer = base_model.get_layer('conv5_block3_out')\n",
    "        resnet_conv_model = tf.keras.Model(\n",
    "            inputs=base_model.input,\n",
    "            outputs=[conv_layer.output, base_model.output]\n",
    "        )\n",
    "        \n",
    "        conv_output, resnet_output = resnet_conv_model(x, training=False)\n",
    "        tape.watch(conv_output)\n",
    "        \n",
    "        # Through classifier head\n",
    "        x = model.get_layer('dropout')(resnet_output, training=False)\n",
    "        x = model.get_layer('dense')(x)\n",
    "        x = model.get_layer('dropout_1')(x, training=False)\n",
    "        predictions = model.get_layer('dense_1')(x)\n",
    "        \n",
    "        pred_class = tf.argmax(predictions[0])\n",
    "        class_score = predictions[0, pred_class]\n",
    "    \n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(class_score, conv_output)\n",
    "    \n",
    "    # Pool gradients and create heatmap\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    heatmap = tf.reduce_sum(conv_output[0] * pooled_grads, axis=-1)\n",
    "    \n",
    "    # ReLU and normalize\n",
    "    heatmap = tf.maximum(heatmap, 0)\n",
    "    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-8)\n",
    "    \n",
    "    return heatmap.numpy(), pred_class.numpy()\n",
    "\n",
    "# Test\n",
    "test_img = np.expand_dims(x_test_norm[0], 0)\n",
    "heatmap, pred = make_gradcam_heatmap_v3(test_img, model)\n",
    "print(f\"Heatmap shape: {heatmap.shape}\")\n",
    "print(f\"Heatmap range: [{heatmap.min():.3f}, {heatmap.max():.3f}]\")\n",
    "print(f\"Predicted class: {CLASS_NAMES[pred]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e03dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Save state before kernel restart\n",
    "# Run this cell to save state if you need to restart and resume later\n",
    "\n",
    "import pickle\n",
    "\n",
    "model.save(PATHS['models'] / 'resnet50_224_finetuned_current.keras')\n",
    "\n",
    "with open(PATHS['models'] / 'training_history.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'phase1': history_phase1.history,\n",
    "        'phase2': history_phase2.history\n",
    "    }, f)\n",
    "\n",
    "np.savez(PATHS['models'] / 'evaluation_state.npz',\n",
    "         y_pred=y_pred,\n",
    "         y_pred_probs=y_pred_probs,\n",
    "         y_true=y_true,\n",
    "         x_test_norm=x_test_norm)\n",
    "\n",
    "print(f\"State saved to: {PATHS['models']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grad-CAM for sample predictions\n",
    "def display_gradcam(img, heatmap, pred_class, true_class, confidence, alpha=0.4):\n",
    "    \"\"\"Overlay heatmap on image with prediction info.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f'True: {CLASS_NAMES[true_class]}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Heatmap\n",
    "    axes[1].imshow(heatmap, cmap='jet')\n",
    "    axes[1].set_title('Grad-CAM Heatmap')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    heatmap_resized = np.uint8(255 * heatmap)\n",
    "    heatmap_resized = np.array(Image.fromarray(heatmap_resized).resize((32, 32)))\n",
    "    heatmap_colored = plt.cm.jet(heatmap_resized / 255.0)[:, :, :3]\n",
    "    overlay = (1 - alpha) * img + alpha * heatmap_colored\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(f'Pred: {CLASS_NAMES[pred_class]} ({confidence:.1%})')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "from PIL import Image\n",
    "print(\"display_gradcam function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ffa57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the last conv layer in ResNet50 for Grad-CAM\n",
    "# ResNet50 structure: conv5_block3_out is the last conv layer\n",
    "base_model = model.get_layer('resnet50')\n",
    "last_conv_layer = 'conv5_block3_out'\n",
    "\n",
    "# Select sample images: 3 correct, 3 incorrect predictions\n",
    "correct_idx = np.where(y_pred == y_true)[0][:3]\n",
    "incorrect_idx = np.where(y_pred != y_true)[0][:3]\n",
    "sample_indices = np.concatenate([correct_idx, incorrect_idx])\n",
    "\n",
    "print(f\"Last conv layer: {last_conv_layer}\")\n",
    "print(f\"Sample indices: {sample_indices}\")\n",
    "print(f\"  Correct: {correct_idx}\")\n",
    "print(f\"  Incorrect: {incorrect_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f233cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grad-CAM visualizations for sample images\n",
    "fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    img = x_test_norm[idx]\n",
    "    img_batch = np.expand_dims(img, 0)\n",
    "    \n",
    "    # Generate heatmap\n",
    "    heatmap, pred = make_gradcam_heatmap_v3(img_batch, model)\n",
    "    \n",
    "    # Resize heatmap to image size (7x7 -> 32x32)\n",
    "    heatmap_resized = np.array(Image.fromarray(np.uint8(255 * heatmap)).resize((32, 32))) / 255.0\n",
    "    \n",
    "    # Row 0: Original image\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f'True: {CLASS_NAMES[y_true[idx]]}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Row 1: Grad-CAM overlay\n",
    "    heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]\n",
    "    overlay = 0.6 * img + 0.4 * heatmap_colored\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    is_correct = y_pred[idx] == y_true[idx]\n",
    "    title_color = 'green' if is_correct else 'red'\n",
    "    conf = y_pred_probs[idx, y_pred[idx]]\n",
    "    \n",
    "    axes[1, i].imshow(overlay)\n",
    "    axes[1, i].set_title(f'Pred: {CLASS_NAMES[y_pred[idx]]} ({conf:.0%})', \n",
    "                         fontsize=10, color=title_color)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Add labels\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Grad-CAM', fontsize=12)\n",
    "\n",
    "plt.suptitle('Grad-CAM: Model Attention (Green=Correct, Red=Incorrect)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS['figures'] / 'gradcam_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {PATHS['figures'] / 'gradcam_samples.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM analysis: Most confused class pairs\n",
    "# From confusion matrix: cat<->dog, bird<->deer were most confused\n",
    "\n",
    "# Find misclassified examples for these pairs\n",
    "cat_as_dog = np.where((y_true == 3) & (y_pred == 5))[0][:2]  # cat predicted as dog\n",
    "dog_as_cat = np.where((y_true == 5) & (y_pred == 3))[0][:2]  # dog predicted as cat\n",
    "bird_as_deer = np.where((y_true == 2) & (y_pred == 4))[0][:2]  # bird predicted as deer\n",
    "\n",
    "confused_pairs = [\n",
    "    ('Cat→Dog', cat_as_dog),\n",
    "    ('Dog→Cat', dog_as_cat),\n",
    "    ('Bird→Deer', bird_as_deer)\n",
    "]\n",
    "\n",
    "print(\"Confusion analysis samples:\")\n",
    "for name, indices in confused_pairs:\n",
    "    print(f\"  {name}: {len(indices)} samples found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM for confused class pairs\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "\n",
    "row = 0\n",
    "for name, indices in confused_pairs:\n",
    "    for col, idx in enumerate(indices):\n",
    "        img = x_test_norm[idx]\n",
    "        img_batch = np.expand_dims(img, 0)\n",
    "        \n",
    "        # Generate heatmap\n",
    "        heatmap, _ = make_gradcam_heatmap_v3(img_batch, model)\n",
    "        heatmap_resized = np.array(Image.fromarray(np.uint8(255 * heatmap)).resize((32, 32))) / 255.0\n",
    "        \n",
    "        # Original\n",
    "        axes[row, col*2].imshow(img)\n",
    "        axes[row, col*2].set_title(f'True: {CLASS_NAMES[y_true[idx]]}', fontsize=9)\n",
    "        axes[row, col*2].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        heatmap_colored = plt.cm.jet(heatmap_resized)[:, :, :3]\n",
    "        overlay = 0.6 * img + 0.4 * heatmap_colored\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "        \n",
    "        conf = y_pred_probs[idx, y_pred[idx]]\n",
    "        axes[row, col*2+1].imshow(overlay)\n",
    "        axes[row, col*2+1].set_title(f'Pred: {CLASS_NAMES[y_pred[idx]]} ({conf:.0%})', fontsize=9, color='red')\n",
    "        axes[row, col*2+1].axis('off')\n",
    "    \n",
    "    # Row label\n",
    "    axes[row, 0].set_ylabel(name, fontsize=11, rotation=0, labelpad=40, va='center')\n",
    "    row += 1\n",
    "\n",
    "plt.suptitle('Grad-CAM: Confused Class Pairs Analysis', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PATHS['figures'] / 'gradcam_confusion_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {PATHS['figures'] / 'gradcam_confusion_analysis.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1420847",
   "metadata": {},
   "source": [
    "**GRAD-CAM ANALYSIS SUMMARY**\n",
    "\n",
    "Key Observations:\n",
    "1. Correct predictions: Model focuses on object body/shape\n",
    "2. Cat↔Dog confusion: Similar body shapes, model focuses on fur texture\n",
    "3. Bird↔Deer confusion: Both have similar silhouettes in natural settings\n",
    "\n",
    "Model Attention Patterns:\n",
    "- Vehicles (ship, truck, auto): Clear edges and distinct shapes\n",
    "- Animals: Overlapping features cause confusion\n",
    "- Small 32x32 images lose discriminative details after 224x224 upscaling\n",
    "\n",
    "Limitations identified:\n",
    "- Heatmap resolution (7x7) is coarse for 32x32 input\n",
    "- Interpolation artifacts from resizing affect feature quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a2eb6",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "| Metric | Phase 1 (Frozen) | Phase 2 (Fine-tuned) | Target |\n",
    "|--------|------------------|----------------------|--------|\n",
    "| Val Accuracy | 10.3% | 49.7% | 65% |\n",
    "| Test Accuracy | ~10% | 48.6% | 65% |\n",
    "| Training Time | 16 min | 52 min | - |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Transfer learning requires input size compatibility**: ResNet50 pretrained on 224x224 images produced random-chance accuracy on native 32x32 CIFAR-10. Resizing was essential.\n",
    "\n",
    "2. **Fine-tuning dramatically improves performance**: Unfreezing conv5 block improved accuracy from 10% to 49% (5x improvement).\n",
    "\n",
    "3. **Architecture mismatch limits ceiling**: Upscaling 32x32→224x224 introduces interpolation artifacts. A smaller architecture (ResNet18, MobileNet) or CIFAR-specific model would be more appropriate.\n",
    "\n",
    "4. **Class confusion follows visual similarity**: Cat/dog and bird/deer pairs were most confused due to similar shapes and textures.\n",
    "\n",
    "### Recommendations for Future Work\n",
    "\n",
    "- Use a model pretrained on smaller images or train from scratch\n",
    "- Increase training data (full 50K CIFAR-10 training set)\n",
    "- Try architectures designed for small images (e.g., ResNet-20 for CIFAR)\n",
    "- Apply mixup or cutout augmentation for better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab0ffb",
   "metadata": {},
   "source": [
    "**CIFAR-10 Classification Project – Final Summary**\n",
    "\n",
    "---\n",
    "\n",
    "**Results**\n",
    "\n",
    "- **Test Accuracy:** 48.5%\n",
    "- **Target:** 65%\n",
    "- **Performance Gap:** 16.5 percentage points\n",
    "\n",
    "---\n",
    "\n",
    "**Training Time**\n",
    "\n",
    "- **Phase 1 (Frozen Base):** ~16 minutes  \n",
    "- **Phase 2 (Fine-Tuning):** ~52 minutes  \n",
    "- **Total Training Time:** ~68 minutes\n",
    "\n",
    "---\n",
    "\n",
    "**Model**\n",
    "\n",
    "- **Architecture:** ResNet50 with custom classification head  \n",
    "- **Input Resolution:** 32×32 → resized to 224×224  \n",
    "- **Fine-Tuned Layers:** Conv5 block  \n",
    "- **Total Parameters:** 24,114,826\n",
    "\n",
    "---\n",
    "\n",
    "**Saved Artifacts**\n",
    "\n",
    "- **Raw Data:** 0 files  \n",
    "- **Processed Data:** 0 files  \n",
    "- **Models:** 6 files  \n",
    "- **Figures:** 8 files\n",
    "\n",
    "---\n",
    "\n",
    "**Figures Generated**\n",
    "\n",
    "- `augmentation_examples.png`\n",
    "- `class_distribution.png`\n",
    "- `confusion_matrix.png`\n",
    "- `gradcam_confusion_analysis.png`\n",
    "- `gradcam_samples.png`\n",
    "- `phase1_training_curves.png`\n",
    "- `phase2_training_curves.png`\n",
    "- `sample_images.png`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
